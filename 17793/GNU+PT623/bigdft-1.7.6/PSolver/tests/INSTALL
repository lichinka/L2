/*! @file
@brief Installation of the PSolver library

@page INSTALLATION-PSOLVER Installation instructions of the PSolver library

This Poisson Solver is conceived as a module, compiled from the file 'Poisson_Solver.f90'
For using it inside a separate program, please read the documentation in such file.

It exists also a separate testing program and a bench test that can be used to familiarize with
the code (and by me for performing non-regression tests)

The instruction for running this are the following:

-# Compile the test program with
   @verbatim
   make PSolver
   @endverbatim
   of course you can specify your (mpi)f90 compiler by suitably changing the Makefile.
-# Run the test program by typing
   @verbatim
   <instruction for running your executable> PSolver 'n01' 'n02' 'n03' 'ixc' 'geocode' 'datacode'
   @endverbatim
   for the description of the arguments, see documentation of PSolver routine
-# Then you get something like that:
   @verbatim
   > ompirun -np 1 PSolver 128 128 128 0 F D
   PSolver, free BC:  128 128 128 processes 1
   Calculating Poisson solver kernel, free BC... done.
   Allocate words for kernel 2146689
   Grid Dimensions:  128  128  128
   PSolver, free  BC, dimensions:   128  128  128   proc  1   ixc:  0 ... done.
   --------------------
   Parallel calculation
   eht, exc, vxc:  3.989422803966E-01  0.000000000000E+00  0.000000000000E+00
    Max diff at: 5 3 4
       Max diff:  1.414596495497E-10
         result:  1.231566544196E-01
       original:  1.231566545611E-01
   Calculating Poisson solver kernel, free BC... done.
   Allocate words for kernel 2146689
   Grid Dimensions:  128  128  128
   PSolver, free  BC, dimensions:   128  128  128   proc  1   ixc:  0 ... done.
   ------------------
   Serial Calculation
   eht, exc, vxc:  3.989422803966E-01  0.000000000000E+00  0.000000000000E+00
    Max diff at: 5 3 4
        Max diff:  1.414596495497E-10
          result:  1.231566544196E-01
        original:  1.231566545611E-01
   ------------------
   difference parallel-serial, at1 1 1
       Max diff:  0.0000E+00
       parallel:  1.1730E-01
         serial:  1.1730E-01
   energy_diffs:  0.0000E+00  0.0000E+00  0.0000E+00
   @endverbatim
   Where the differences between the computation and the analytic results are displayed.


A report is provided in the file fort.99, which can be read like that:
@verbatim
  > more fort.99
F  128 128 128 7.81E-02 1.415E-10 1.94E+00 1.76E-01 8.90E+00 6.00E-02 17.6  1.6 80.9 0.5 0.00E+00
^ |dimensions|   ^          ^     |time 1)computation 2)communication| percentage of    | diff.
|              hgrid    accuracy  |3)kernel 4)data distribution, XC  |  previous parts  | par-ser
geocode 
@endverbatim

Note that the timings for initializing the test arrays is not taken into account.
The difference between the analytic and the computed results is expected to be little ONLY 
in the ixc=0 case. Otherwise, the difference berween the parallel and the serial calculation is a
good indicator.

Also a bench that calculates the accuracy wrt the grid spacing is provided.
You can run that by typing:
@verbatim
./bench.csh 'start' 'step' 'end' 'geocode' 'datacode'
@endverbatim
where 'start' and 'end' indicate the range of the dimension spanned with step 'step'.
Of course the mpi instruction must be properly adapted to your system in the script.

The output files of this bench may be used for testing.
Some reference files (.ref) are added to control the non-regression. 
These are runs performed with ixc=0.

A file time.par is produced at each run to indicate the time for each step.
The resolution of the timing is one millisecond.

Hope it's enough, feel free to contact me at luigi.genovese@cea.fr for any 
problems/comments/suggestions

Bye,

LG
*/
